{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0QDPCG0vTTZ"
   },
   "source": [
    "# Part 1. Authentication in a service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnfogDHdvTTf"
   },
   "source": [
    "## 1.1. What do you store in your Google Drive?\n",
    "\n",
    "Sometimes it can be quite troublesome to crawl web data - for example, when you can't just collect data from web-pages because the authentification to a website is required. Today's tutorial is about a dataset of special type - namely, Google Drive data. You will need to get access to the system using OAuth protocol, download and parse files of different types.\n",
    "\n",
    "Plan. \n",
    "1. Download [this little archive](https://drive.google.com/open?id=1Xji4A_dEAm_ycnO0Eq6vxj7ThcqZyJZR), **unzip** it and place the folder anywhere inside your Google Drive. You should get a subtree of 6 folders with files of different types: presentations, pdf-files, texts, and even code.\n",
    "2. Go to [Google Drive API](https://developers.google.com/drive/api/v3/quickstart/python) documentation, read [intro](https://developers.google.com/drive/api/v3/about-sdk) and learn how to [search for files](https://developers.google.com/drive/api/v3/reference/files/list) and [download](https://developers.google.com/drive/api/v3/manage-downloads) them. Pay attention, that  working at `localhost` (jupyter) and at `google colab` can be slighty different. We expect you to run from localhost.\n",
    "3. Learn how to open from python such files as [pptx](https://python-pptx.readthedocs.io/en/latest/user/quickstart.html), pdf, docx or even use generalized libraries like [textract](https://textract.readthedocs.io/en/stable/index.html), save internal text in a file near.\n",
    "4. Write a code with returns names (with paths) of files for a given substring. Test on these queries.\n",
    "```\n",
    "segmentation\n",
    "algorithm\n",
    "classifer\n",
    "printf\n",
    "predecessor\n",
    "Шеннон\n",
    "Huffman\n",
    "function\n",
    "constructor\n",
    "machine learning\n",
    "dataset\n",
    "Протасов\n",
    "Protasov\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNvIDD69vTTh"
   },
   "source": [
    "### 1.1.1. Access GDrive ###\n",
    "\n",
    "Below is the example of how you can oranize your code - it's fine if you change it.\n",
    "\n",
    "Let's extract the list of all files that are contained (recursively) in t\n",
    "he folder of interest. In my case, I called it `air_oauth_folder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYYqCLnhvTTi",
    "outputId": "e89a5ca2-3bd5-42ef-c6b7-a603df629183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: google-api-python-client in ./.local/lib/python3.8/site-packages (1.12.8)\n",
      "Requirement already up-to-date: google-auth-httplib2 in ./.local/lib/python3.8/site-packages (0.0.4)\n",
      "Requirement already up-to-date: google-auth-oauthlib in ./.local/lib/python3.8/site-packages (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in ./.local/lib/python3.8/site-packages (from google-api-python-client) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.16.0 in ./.local/lib/python3.8/site-packages (from google-api-python-client) (1.23.0)\n",
      "Requirement already satisfied, skipping upgrade: six<2dev,>=1.13.0 in ./.local/lib/python3.8/site-packages (from google-api-python-client) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.15.0 in /usr/lib/python3/dist-packages (from google-api-python-client) (0.18.1)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2dev,>=1.21.0 in ./.local/lib/python3.8/site-packages (from google-api-python-client) (1.25.1)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in ./.local/lib/python3.8/site-packages (from google-auth-oauthlib) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/lib/python3/dist-packages (from google-auth>=1.16.0->google-api-python-client) (49.3.1)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in ./.local/lib/python3.8/site-packages (from google-auth>=1.16.0->google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in ./.local/lib/python3.8/site-packages (from google-auth>=1.16.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.5\" in ./.local/lib/python3.8/site-packages (from google-auth>=1.16.0->google-api-python-client) (4.6)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/lib/python3/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (3.12.3)\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/lib/python3/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /usr/lib/python3/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in ./.local/lib/python3.8/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client) (1.52.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in ./.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "# install some dependencies\n",
    "!pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
    "# !pip install --upgrade six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LadHRxR_vTTj",
    "scrolled": true,
    "outputId": "d2c5c1c8-eb32-459d-d71d-cd38c1008605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=235774715710-tah0thbhuv9frlm8hqg4rjgrr7q8b0t4.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A45739%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&state=ByXjUfw7YXxRu9WmCoM1NFrN6RPgV5&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pickle\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import io\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "\n",
    "\"\"\"Shows basic usage of the Drive v3 API.\n",
    "Prints the names and ids of the first 10 files the user has access to.\n",
    "\"\"\"\n",
    "\n",
    "# If modifying these scopes, delete the file token.pickle.\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "config = {\"installed\":{\"client_id\":\"235774715710-tah0thbhuv9frlm8hqg4rjgrr7q8b0t4.apps.googleusercontent.com\",\"project_id\":\"assignment2-1611750152663\",\"auth_uri\":\"https://accounts.google.com/o/oauth2/auth\",\"token_uri\":\"https://oauth2.googleapis.com/token\",\"auth_provider_x509_cert_url\":\"https://www.googleapis.com/oauth2/v1/certs\",\"client_secret\":\"0g0RD33YKPxevVp0U3LJ33K1\",\"redirect_uris\":[\"urn:ietf:wg:oauth:2.0:oob\",\"http://localhost\"]}}\n",
    "creds = None\n",
    "\n",
    "flow = InstalledAppFlow.from_client_config(\n",
    "            config, SCOPES)\n",
    "creds = flow.run_local_server(port=0)\n",
    "\n",
    "drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "def gdrive_get_all_files_in_folder(folder_name):\n",
    "    listofFiles = []\n",
    "\n",
    "    page_token = None\n",
    "    while True:\n",
    "        response = drive_service.files().list(q=\"name = '\"+folder_name+\"'\",\n",
    "                                              spaces='drive',\n",
    "                                              fields='nextPageToken, files(id, name)',\n",
    "                                              pageToken=page_token).execute()\n",
    "\n",
    "        for file in response.get('files', []):               \n",
    "                file_id = file.get('id')\n",
    "                results = drive_service.files().list(q = \"'\" + file_id + \"' in parents\", \n",
    "                                                     spaces='drive',\n",
    "                                                  fields='nextPageToken, files(id, name)',\n",
    "                                                  pageToken=page_token).execute()\n",
    "                items = results.get('files', [])\n",
    "                i = len(items)\n",
    "                while items != []:\n",
    "                    ob1 = items.pop()\n",
    "                    if \".\" in ob1['name']:\n",
    "                        listofFiles.append([ob1['id'], ob1['name']])\n",
    "                    else:\n",
    "                        results2 = drive_service.files().list(q = \"'\" + ob1['id'] + \"' in parents\", \n",
    "                                                             spaces='drive',\n",
    "                                                             fields='nextPageToken, files(id, name)',\n",
    "                                                             pageToken=page_token).execute()\n",
    "                        items2 = results2.get('files', [])\n",
    "                        items.extend(items2)\n",
    "\n",
    "        page_token = response.get('nextPageToken', None)\n",
    "        if page_token is None:\n",
    "            break\n",
    "    return listofFiles\n",
    "\n",
    "def gdrive_download_file(file, path_to_save):\n",
    "#     print(file)\n",
    "    file_id = file[0] \n",
    "    filename = str(path_to_save) +\"/\"+ str(file[1])\n",
    "    \n",
    "    if not os.path.exists(path_to_save):\n",
    "        os.makedirs(path_to_save)\n",
    "    \n",
    "    request = drive_service.files().get_media(fileId=file_id)\n",
    "\n",
    "    fh = io.FileIO(filename, 'wb')\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "#         print (\"Download %d%%.\" % int(status.progress() * 100))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKogbzV1vTTk"
   },
   "outputs": [],
   "source": [
    "folder_of_interest = 'data11'\n",
    "files = gdrive_get_all_files_in_folder(folder_of_interest)\n",
    "\n",
    "test_dir = \"test_files2\"\n",
    "for item in files:\n",
    "    gdrive_download_file(item, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlNh1O6fvTTl"
   },
   "source": [
    "### 1.1.2. Tests ###\n",
    "Please fill free to change function signatures and behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JP46nZxdvTTl",
    "outputId": "4659fe35-2068-44df-c00e-66ccd3e74e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_files: 34\n",
      "file here means id and name, e.g.:  ['1RNa8ZjygiROaF95n0wOv7NJYas_oc1J1', '[DM]-Course Description.docx']\n"
     ]
    }
   ],
   "source": [
    "assert len(files) == 34, 'Number of files is incorrect'\n",
    "print('n_files:', len(files))\n",
    "\n",
    "print(\"file here means id and name, e.g.: \", files[0])\n",
    "\n",
    "gdrive_download_file(files[0], '.')\n",
    "\n",
    "import os.path\n",
    "assert os.path.isfile(os.path.join('.', files[0][1])), \"File is not downloaded correctly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lI-NtmMYvTTm"
   },
   "source": [
    "## 1.2. Read files content\n",
    "### 1.2.1. Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10NLScqkvTTn",
    "outputId": "d6d77646-d742-4450-d099-14f2ba05c558"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textract in ./.local/lib/python3.8/site-packages (1.6.3)\n",
      "Requirement already satisfied: docx2txt==0.8 in ./.local/lib/python3.8/site-packages (from textract) (0.8)\n",
      "Requirement already satisfied: SpeechRecognition==3.8.1 in ./.local/lib/python3.8/site-packages (from textract) (3.8.1)\n",
      "Requirement already satisfied: chardet==3.0.4 in /usr/lib/python3/dist-packages (from textract) (3.0.4)\n",
      "Requirement already satisfied: python-pptx==0.6.18 in ./.local/lib/python3.8/site-packages (from textract) (0.6.18)\n",
      "Requirement already satisfied: beautifulsoup4==4.8.0 in ./.local/lib/python3.8/site-packages (from textract) (4.8.0)\n",
      "Requirement already satisfied: argcomplete==1.10.0 in ./.local/lib/python3.8/site-packages (from textract) (1.10.0)\n",
      "Requirement already satisfied: EbookLib==0.17.1 in ./.local/lib/python3.8/site-packages (from textract) (0.17.1)\n",
      "Requirement already satisfied: extract-msg==0.23.1 in ./.local/lib/python3.8/site-packages (from textract) (0.23.1)\n",
      "Requirement already satisfied: pdfminer.six==20181108 in ./.local/lib/python3.8/site-packages (from textract) (20181108)\n",
      "Collecting six==1.12.0\n",
      "  Using cached six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: xlrd==1.2.0 in ./.local/lib/python3.8/site-packages (from textract) (1.2.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in ./.local/lib/python3.8/site-packages (from python-pptx==0.6.18->textract) (1.3.7)\n",
      "Requirement already satisfied: lxml>=3.1.0 in ./.local/lib/python3.8/site-packages (from python-pptx==0.6.18->textract) (4.6.2)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /usr/lib/python3/dist-packages (from python-pptx==0.6.18->textract) (7.2.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in ./.local/lib/python3.8/site-packages (from beautifulsoup4==4.8.0->textract) (2.1)\n",
      "Requirement already satisfied: olefile==0.46 in /usr/lib/python3/dist-packages (from extract-msg==0.23.1->textract) (0.46)\n",
      "Requirement already satisfied: tzlocal==1.5.1 in ./.local/lib/python3.8/site-packages (from extract-msg==0.23.1->textract) (1.5.1)\n",
      "Requirement already satisfied: imapclient==2.1.0 in ./.local/lib/python3.8/site-packages (from extract-msg==0.23.1->textract) (2.1.0)\n",
      "Requirement already satisfied: pycryptodome in ./.local/lib/python3.8/site-packages (from pdfminer.six==20181108->textract) (3.9.9)\n",
      "Requirement already satisfied: sortedcontainers in ./.local/lib/python3.8/site-packages (from pdfminer.six==20181108->textract) (2.3.0)\n",
      "Requirement already satisfied: pytz in /usr/lib/python3/dist-packages (from tzlocal==1.5.1->extract-msg==0.23.1->textract) (2020.1)\n",
      "\u001B[31mERROR: launchpadlib 1.10.13 requires testresources, which is not installed.\u001B[0m\n",
      "\u001B[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001B[0m\n",
      "\u001B[31mERROR: google-api-core 1.25.1 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001B[0m\n",
      "Installing collected packages: six\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n",
      "      Successfully uninstalled six-1.15.0\n",
      "Successfully installed six-1.12.0\n",
      "Requirement already satisfied: pydub in ./.local/lib/python3.8/site-packages (0.24.1)\n",
      "Requirement already satisfied: moviepy in ./.local/lib/python3.8/site-packages (1.0.3)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in ./.local/lib/python3.8/site-packages (from moviepy) (4.51.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0; python_version >= \"3.4\" in ./.local/lib/python3.8/site-packages (from moviepy) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17.3; python_version != \"2.7\" in ./.local/lib/python3.8/site-packages (from moviepy) (1.18.2)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.8/dist-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5; python_version >= \"3.4\" in ./.local/lib/python3.8/site-packages (from moviepy) (2.9.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/lib/python3/dist-packages (from moviepy) (2.23.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in ./.local/lib/python3.8/site-packages (from moviepy) (0.1.9)\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from imageio<3.0,>=2.5; python_version >= \"3.4\"->moviepy) (7.2.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install dependencies\n",
    "!pip install textract\n",
    "!pip install pydub\n",
    "!pip install moviepy\n",
    "\n",
    "import os\n",
    "# cmd1 = \"apt -qq install -y sox\" # If there is a sox error, kindly run this command in os.popen\n",
    "command = \"apt-get -y install ffmpeg\"\n",
    "# os.popen(\"sudo -S %s\"%(command), 'w').write('your_system_password') # please put your password here to install this library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a0ME-QLvTTn"
   },
   "source": [
    "For windows please refer to \n",
    "- https://textract.readthedocs.io/en/latest/installation.html#don-t-see-your-operating-system-installation-instructions-here\n",
    "\n",
    "- https://www.xpdfreader.com/download.html\n",
    "\n",
    "ALSO BE CAREFUL WITH SPACES IN NAMES. Better save without spaces!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkUWHUfsvTTo"
   },
   "outputs": [],
   "source": [
    "import textract\n",
    "import speech_recognition as sr\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pydub import AudioSegment # uses FFMPEG\n",
    "from moviepy.editor import *\n",
    "\n",
    "def pdfparser(data):\n",
    "\n",
    "    fp = open(data, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    # Process each page contained in the document.\n",
    "\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "\n",
    "#     print(data)\n",
    "    if \"ﬁ\" in data:\n",
    "        data = data.replace(\"ﬁ\", \"fi\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def processAudio(filepath, chunksize=60000):\n",
    "    #0: load mp3\n",
    "    sound = AudioSegment.from_mp3(filepath)\n",
    "    \n",
    "    path = filepath[:-3]\n",
    "    \n",
    "\n",
    "    #1: spliting the file into 60s chunks\n",
    "    def divide_chunks(sound, chunksize):\n",
    "        # looping till length l\n",
    "        for i in range(0, len(sound), chunksize):\n",
    "            yield sound[i:i + chunksize]\n",
    "    chunks = list(divide_chunks(sound, chunksize))\n",
    "#     print(f\"{len(chunks)} chunks of {chunksize/1000}s each\")\n",
    "\n",
    "    r = sr.Recognizer()\n",
    "    #2: per chunk, save to wav, then read and run through recognize_google()\n",
    "    string_index = {}\n",
    "    i=0\n",
    "    textfromaudio = \"\"\n",
    "    for index,chunk in enumerate(chunks):\n",
    "        chunk.export(path+'1.wav', format='wav')\n",
    "        with sr.AudioFile(path+'1.wav') as source:\n",
    "            audio = r.record(source)\n",
    "        \n",
    "        s = \"\"\n",
    "#         s = textract.process(path+'1.wav', lang='rus')\n",
    "        \n",
    "        os.remove(path+'1.wav')\n",
    "        try:\n",
    "            s = r.recognize_google(audio, language='ru-RU')\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google service; {0}\".format(e))\n",
    "\n",
    "        textfromaudio += \" \" + str(s)\n",
    "        \"\"\"\n",
    "        the audio is lengthy, this check is for saving the time, \n",
    "        if you want to transcribe the whole audio just comment this check\n",
    "        \"\"\"\n",
    "        i += 1\n",
    "        if i == 1: \n",
    "            break\n",
    "    \n",
    "    return textfromaudio\n",
    "\n",
    "\n",
    "def get_file_strings(path):\n",
    "    # - textract is not able to parse everything\n",
    "    # Take care of non-text data too\n",
    "    texts = \"\"\n",
    "    if \".mp3\" in path:\n",
    "        texts = \"\"\n",
    "#         print(\"audio\"+path)\n",
    "#         texts = str(processAudio(path))\n",
    "    elif \"avi\" in path:\n",
    "        texts = \"\"\n",
    "#         videoclip = VideoFileClip(path)\n",
    "#         audioclip = videoclip.audio\n",
    "\n",
    "#         if audioclip == None:\n",
    "#             texts = \"\"\n",
    "#         else:\n",
    "#             try:\n",
    "#                 texts = r.recognize_google(audioclip)\n",
    "#             except sr.UnknownValueError:\n",
    "#                 texts = \"\"\n",
    "#                 print(\"Could not understand audio\")\n",
    "#             except sr.RequestError as e:\n",
    "#                 texts = \"\"\n",
    "#                 print(\"Could not request results from Google service; {0}\".format(e))\n",
    "    elif \".cpp\" in path:\n",
    "        soup=BeautifulSoup(open(path), 'lxml')\n",
    "        texts = soup.get_text()\n",
    "    elif \".c\" in path:\n",
    "        soup=BeautifulSoup(open(path), 'lxml')\n",
    "        texts = soup.get_text()\n",
    "    elif \".js\" in path:\n",
    "        soup=BeautifulSoup(open(path), 'lxml')\n",
    "        texts = soup.get_text()\n",
    "    elif \".html\" in path:\n",
    "        soup=BeautifulSoup(open(path), \"html.parser\")\n",
    "        texts = soup.get_text()\n",
    "        \n",
    "    elif \".pdf\" in path:\n",
    "        texts = pdfparser(path)\n",
    "    elif \".txt\" in path:\n",
    "        try:\n",
    "            f = open(path, encoding='utf-8', mode='r')\n",
    "            text = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            f = open(path, \"r\", encoding = \"ISO-8859-1\")\n",
    "            text = f.read()\n",
    "        texts = str(text)\n",
    "    else:\n",
    "        try:\n",
    "            text = textract.process(path)\n",
    "        except UnicodeDecodeError:\n",
    "            f = open(path, \"r\", encoding = \"ISO-8859-1\")\n",
    "            text =f.read()\n",
    "\n",
    "        texts = str(text)\n",
    "    if texts == \"\":\n",
    "#         print(path)\n",
    "        return None\n",
    "    else:\n",
    "#         print(len(texts))\n",
    "        texts = str(texts).replace('\\\\n', '\\n').replace('\\\\r', '').split('\\n')\n",
    "\n",
    "    return str(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4KUgGwwvTTo"
   },
   "outputs": [],
   "source": [
    "# creating dictionary of parsed files\n",
    "test_dir = \"test_files2\"\n",
    "files_data = dict()\n",
    "for file in os.scandir(test_dir):\n",
    "    strings = get_file_strings(file.path)\n",
    "    if strings:\n",
    "        files_data[file.name] = strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AD2zom7FvTTo"
   },
   "source": [
    "### 1.2.2. Tests for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5-iyqYCvTTp",
    "outputId": "46a2d9fb-7afb-463e-eb6f-cd447f43a6c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "assert len(files_data) == 31 # there should be 33 files\n",
    "print(len(files_data))\n",
    "\n",
    "assert \"Protasov\" in get_file_strings(os.path.join(test_dir, 'at least this file.txt')), \"TXT File parsed incorrectly\"\n",
    "assert \"A. Image classification\" in get_file_strings(os.path.join(test_dir, 'deep-features-scene (1).pdf')), \"PDF File parsed incorrectly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRvFHo3B39ip",
    "outputId": "03554fa8-3a25-4116-ebb0-2909642ad44e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "# For making procedure faster\n",
    "\n",
    "import pickle\n",
    "\n",
    "# a_file = open(\"data1.pkl\", \"wb\")\n",
    "# pickle.dump(files_data, a_file)\n",
    "# a_file.close()\n",
    "\n",
    "a_file = open(\"data.pkl\", \"rb\")\n",
    "files_data1 = pickle.load(a_file)\n",
    "print(len(files_data1))\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPHd1ZK3vTTp"
   },
   "source": [
    "## 1.3. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRzRGfgDvTTp"
   },
   "outputs": [],
   "source": [
    "def find(query):\n",
    "    ret = []\n",
    "    notfound = [\"Couldn't\", \"Find\"]\n",
    "    for key in files_data:\n",
    "        ele = files_data[key]\n",
    "        if isinstance(ele, dict):\n",
    "            for k, v in ele.items():\n",
    "                text = v.lower()\n",
    "                if query.lower() in text:\n",
    "                    slideId = \"Presentation: \"+str(key) + \", Slide No: \"+ str(k)\n",
    "                    ret.append(slideId)\n",
    "        else:\n",
    "            text = ele.lower()\n",
    "            if query.lower() in text:\n",
    "                ret.append(key)\n",
    "    if len(ret) <= 1:\n",
    "        ret.extend(notfound)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9hHu4XwvTTp"
   },
   "outputs": [],
   "source": [
    "queries = [\"segmentation\", \"algorithm\", \"printf\", \"predecessor\", \"Huffman\",\n",
    "           \"function\", \"constructor\", \"machine learning\", \"dataset\", \"Protasov\"]\n",
    "\n",
    "for query in queries:\n",
    "    r = find(query)\n",
    "#     print(\"Results for: \", query)\n",
    "#     print(\"\\t\", r)\n",
    "    assert len(r) > 0, \"Query should return at least 1 document\"\n",
    "    assert len(r) > 1, \"Query should return at least 2 documents\"\n",
    "    assert \"at least this file.txt\" in r, \"This file has all the queries. It should be in a result\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8vuV5WOvTTq"
   },
   "source": [
    "# 2. Parse me if you can #\n",
    "\n",
    "Sometimes when crawling we have to parse websites that turn out to be SaaS - i.e., there is a special JS application which renders documents and which is downloaded first. Therefore, data that is to be rendered initially comes in a proprietary format. One of the examples is Google Drive. Last time we downladed and parsed some files from GDrive, however, we didn't parse GDrive-specific file formats, such as google sheets or google slides.\n",
    "\n",
    "Today we will learn to obtain and parse such data using Selenium - a special framework for testing web-applications.\n",
    "\n",
    "## 2.1. Getting started\n",
    "\n",
    "Let's try to load and parse the page the way we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qj9djQ5vvTTq",
    "outputId": "2e9fdca6-22de-447d-90ea-3ede7bf6b5d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Не удалось открыть файл, поскольку в вашем браузере отключено использование JavaScript. Включите его и перезагрузите страницу.Некоторые функции PowerPoint не поддерживаются в Google Презентациях. Они будут удалены, если вы измените документ.Подробнее…6. Approximate nearest neighbours search 2. Trees   Смотреть  Открыть доступВойтиИспользуемая вами версия браузера больше не поддерживается. Установите поддерживаемую версию браузера.Закрытьdocument.getElementById('docs-unsupported-browser-bar').addEventListener('click', function () {this.parentNode.parentNode.removeChild(this.parentNode);return false;});ФайлПравкаВидСправкаСпециальные возможностиОтладкаНесохраненные изменения: ДискПоследние изменения      Специальные возможности  Только просмотр     DOCS_timing['che'] = new Date().getTime();DOCS_timing['chv'] = new Date().getTime();Презентация в виде HTML(function(){/*\n",
      "\n",
      " Copyright The Closure Library Authors.\n",
      " SPDX-License-Identifier: Apache-2.0\n",
      "*/\n",
      "var a=this||self;function b(){this.g=thi\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "resp = requests.get(\"https://docs.google.com/presentation/d/1LuZvz3axBD8UuHLagdv0EbhsGEWJmpd7gN5KjwYCp9Y/edit?usp=sharing\")\n",
    "soup = BeautifulSoup(resp.text, 'lxml')\n",
    "print(soup.body.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4eVRkOxvTTq"
   },
   "source": [
    "As we see, the output is not what we expect. So, what can we do when a page is not being loaded right away, but is rather rendered by a script? Browser engines can help us get data. Let's try to load the same web-page, but do it in a different way: let's give some time to a browser to load the scripts and run them; and then will work with DOM (Document Object Model), but will get it from browser engine itself, not from BeautifulSoup.\n",
    "\n",
    "Where do we get browser engine from? Simply installing a browser will do the thing. How do we send commands to it from code and retrieve DOM? Service applications called drivers will interpret out commands and translate them into browser actions.\n",
    "\n",
    "\n",
    "For each browser engine suport you will need to:\n",
    "1. install browser itself;\n",
    "2. download 'driver' - binary executable, which passed commands from selenium to browser. E.g. [Gecko == Firefox](https://github.com/mozilla/geckodriver/releases), [ChromeDriver](http://chromedriver.storage.googleapis.com/index.html);\n",
    "3. unpack driver into a folder under PATH environment variable. Or specify exact binary location.\n",
    "\n",
    "### 2.1.1. Download driver\n",
    "\n",
    "And place it in any folder or under PATH env. variable.\n",
    "\n",
    "### 2.1.2. Install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1JxiD0rvTTr",
    "outputId": "66e636eb-bf85-416a-e71e-6fcd746b275b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: selenium in ./.local/lib/python3.8/site-packages (3.141.0)\r\n",
      "Requirement already satisfied, skipping upgrade: urllib3 in /usr/lib/python3/dist-packages (from selenium) (1.25.9)\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install -U selenium\n",
    "import os\n",
    "command = \"apt install firefox-geckodriver\"\n",
    "os.popen(\"sudo -S %s\"%(command), 'w').write('your_password') # please put your password here to install this library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsi3ZEbwvTTs"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hs6yBi9vTTs"
   },
   "source": [
    "### 2.1.3. Launch browser\n",
    "\n",
    "This will open browser window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwTmRBTKvTTs"
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Firefox()\n",
    "# or explicitly\n",
    "# browser = webdriver.Firefox(\n",
    "#     executable_path='geckodriver', \n",
    "#     firefox_binary='C:/Program Files/Mozilla Firefox/firefox.exe'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXivGa_JvTTs"
   },
   "source": [
    "### 2.1.4. Download the page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tp6pj7kwvTTt",
    "outputId": "2228a0ed-9411-4a6c-a7cc-3fa08987524e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements found: 2\n",
      "What if just a silly approach: Forestsofsearchtrees\n",
      "What if a smart approach: Forests of search trees\n"
     ]
    }
   ],
   "source": [
    "# navigate to page\n",
    "browser.get('http://tiny.cc/00dhkz')\n",
    "browser.implicitly_wait(5)  # wait 5 seconds\n",
    "\n",
    "# select all text parts from document\n",
    "elements = browser.find_elements_by_css_selector(\"g.sketchy-text-content-text\")\n",
    "# note that if number differs from launch to launch this means better extend wait time\n",
    "print(\"Elements found:\", len(elements))\n",
    "\n",
    "# oh no! It glues all the words!\n",
    "print(\"What if just a silly approach:\", elements[0].text)\n",
    "\n",
    "# GDrive stores all text blocks word-by-word\n",
    "subnodes = elements[0].find_elements_by_css_selector(\"text\")\n",
    "text = \" \".join(n.text for n in subnodes)\n",
    "print(\"What if a smart approach:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScaRd1IzvTTt"
   },
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnWNOixwvTTt"
   },
   "source": [
    "- Too slow, wait for browser to open, browser to render\n",
    "\n",
    "## 2.2. Headless\n",
    "\n",
    "Browsers (at least [FF](https://developer.mozilla.org/en-US/docs/Mozilla/Firefox/Headless_mode), [Chrome](https://intoli.com/blog/running-selenium-with-headless-chrome/), IE) have headless mode - no window rendering and so on. Means it should work much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auqWdZ8HvTTt"
   },
   "outputs": [],
   "source": [
    "options = webdriver.FirefoxOptions()\n",
    "\n",
    "options.add_argument('-headless')\n",
    "options.add_argument('window-size=1200x600')\n",
    "browser = webdriver.Firefox(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVWkK33EvTTu",
    "outputId": "6af64c2f-a3b5-4dc1-dc50-c4b9a9c9f1a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements found: 110\n",
      "What if just a silly approach: Forestsofsearchtrees\n",
      "What if a smart approach: Forests of search trees\n"
     ]
    }
   ],
   "source": [
    "## SAME CODE\n",
    "\n",
    "# navigate to page\n",
    "browser.get('http://tiny.cc/00dhkz')\n",
    "browser.implicitly_wait(5)  # wait 5 seconds\n",
    "\n",
    "# select all text parts from document\n",
    "elements = browser.find_elements_by_css_selector(\"g.sketchy-text-content-text\")\n",
    "# note that if number differs from launch to launch this means better extend wait time\n",
    "print(\"Elements found:\", len(elements))\n",
    "\n",
    "# oh no! It adds NEW LINE. Behavior differs!!!!\n",
    "print(\"What if just a silly approach:\", elements[0].text)\n",
    "\n",
    "# GDrive stores all text blocks word-by-word\n",
    "subnodes = elements[0].find_elements_by_css_selector(\"text\")\n",
    "text = \" \".join(n.text for n in subnodes)\n",
    "print(\"What if a smart approach:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XRIZUYBvTTu"
   },
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Och2t1y39iu"
   },
   "source": [
    "### 2.2.1. NB \n",
    "Note, that browser behavior differs for the same code!\n",
    "\n",
    "## 2.3. Task \n",
    "Our lectures usually have lot's of links. Here are the links to original (spring 2020) versions of the documents.\n",
    "\n",
    "[4. Vector space](https://docs.google.com/presentation/d/1UxjGZPPrPTM_3lCa_gWTk8yZI_qNmTKwtMxr8JZQCIc/edit?usp=sharing)\n",
    "\n",
    "[6. search trees](https://docs.google.com/presentation/d/1LuZvz3axBD8UuHLagdv0EbhsGEWJmpd7gN5KjwYCp9Y/edit?usp=sharing)\n",
    "\n",
    "[7-8. Web basics](https://docs.google.com/presentation/d/1bgsCgpjMcQmrFpblRI6oH9SnG4bjyo5SzSSdKxxHNlg/edit?usp=sharing)\n",
    "\n",
    "Please complete the following tasks:\n",
    "\n",
    "### 2.3.1. Search for slides with numbers\n",
    "I want to type a word, and it should say which slides of which lecture has this word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3g2hlo2nvTTu"
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "options = webdriver.FirefoxOptions()\n",
    "\n",
    "options.add_argument('-headless')\n",
    "options.add_argument('window-size=1200x600')\n",
    "\n",
    "def getTextAndImgsFromSlides(url):    \n",
    "    slides_text = dict() # dictionary slide_num : slide_text\n",
    "    img_list = [] # list of image urls\n",
    "    slidnum = 1\n",
    "\n",
    "    #TODO: parse google slides and save all text and image urls in slides_text and img_list\n",
    "    # you should get the contents from ALL slides - however, you will see that at one moment \n",
    "    # of time only single slide + few slide previews on the left are visible. To be able to    \n",
    "    # reach all slides you will need to scroll to and click these previews. While slide contents \n",
    "    # can be obtained from previews themselves, speaker notes (which you also have to extract)\n",
    "    # can be viewed only if a particular slide is open.\n",
    "    # to scroll the element of interest into view, use can this: \n",
    "    # browser.execute_script(\"arguments[0].scrollIntoView();\", el)\n",
    "    # to click the element, use can use ActionChains library   \n",
    "    browser = webdriver.Firefox(options = options)\n",
    "    browser.get(url)\n",
    "\n",
    "    browser.find_element_by_css_selector('body').send_keys(Keys.HOME)\n",
    "    browser.implicitly_wait(15)\n",
    "\n",
    "    element = browser.find_element_by_class_name(\"panel-right\")\n",
    "\n",
    "    #First page images are not loaded\n",
    "    images   = element.find_elements_by_css_selector(\"image\")\n",
    "    for image in images:\n",
    "        img_src = image.get_attribute(\"href\")\n",
    "        img_list.append(img_src)\n",
    "\n",
    "    subelem = element.find_elements_by_css_selector(\"svg\")\n",
    "    truncatingelem = []\n",
    "    browser.implicitly_wait(5)\n",
    "    title = browser.title\n",
    "\n",
    "    while subelem !=[]:\n",
    "        elem = subelem.pop()\n",
    "        if elem not in truncatingelem:\n",
    "            if elem.is_displayed():\n",
    "                truncatingelem.append(elem)\n",
    "                texts = element.find_elements_by_css_selector(\"g.sketchy-text-content-text\")\n",
    "\n",
    "                textt = \"\"\n",
    "                for text in texts:\n",
    "                    subnodes = text.find_elements_by_css_selector(\"text\")\n",
    "                    text = \" \".join(n.text for n in subnodes)\n",
    "                    textt += text\n",
    "                if len(textt.strip())>1:\n",
    "                    slides_text[str(slidnum)] = textt\n",
    "                    slidnum += 1\n",
    "\n",
    "                images   = elem.find_elements_by_css_selector(\"image\")\n",
    "                for image in images:\n",
    "                    img_src = image.get_attribute(\"href\")\n",
    "                    img_list.append(img_src)\n",
    "\n",
    "                browser.find_element_by_css_selector('body').send_keys(Keys.DOWN)\n",
    "                tempsubelem = element.find_elements_by_css_selector(\"svg\")\n",
    "\n",
    "                # for counting slides\n",
    "                for tempelem in tempsubelem:\n",
    "                    if tempelem not in truncatingelem and tempelem not in subelem:\n",
    "                        subelem.append(tempelem)\n",
    "    del slides_text[str(len(slides_text))]\n",
    "    return slides_text, img_list, title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPnO-c73vTTv"
   },
   "source": [
    "Parsing three presentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wct_aThPvTTv"
   },
   "outputs": [],
   "source": [
    "links = [\"https://docs.google.com/presentation/d/1UxjGZPPrPTM_3lCa_gWTk8yZI_qNmTKwtMxr8JZQCIc/edit?usp=sharing\", \n",
    "         \"https://docs.google.com/presentation/d/1LuZvz3axBD8UuHLagdv0EbhsGEWJmpd7gN5KjwYCp9Y/edit?usp=sharing\",\n",
    "         \"https://docs.google.com/presentation/d/1bgsCgpjMcQmrFpblRI6oH9SnG4bjyo5SzSSdKxxHNlg/edit?usp=sharing\"]\n",
    "\n",
    "\n",
    "all_imgs = []\n",
    "all_texts = dict()\n",
    "\n",
    "for i, link in enumerate(links):\n",
    "    texts, imgs, title = getTextAndImgsFromSlides(link)\n",
    "    all_texts[str(title)] = texts\n",
    "    files_data[str(title)]=texts \n",
    "    all_imgs.append(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIr0-5aS39iw"
   },
   "outputs": [],
   "source": [
    "# For making procedure faster\n",
    "\n",
    "# print(len(files_data))\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# a_file = open(\"data1.pkl\", \"wb\")\n",
    "# pickle.dump(files_data, a_file)\n",
    "# a_file.close()\n",
    "\n",
    "a_file = open(\"data1.pkl\", \"rb\")\n",
    "files_data = pickle.load(a_file)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiF1gU_jvTTv"
   },
   "source": [
    "### 2.3.2. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NrYj7-TvTTv",
    "outputId": "ee574132-3807-40d3-fe52-bf5ff7c747c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "texts, imgs, title = getTextAndImgsFromSlides('http://tiny.cc/00dhkz')\n",
    "\n",
    "assert len(texts) == 35 # equal to the total number of slides in the presentation \n",
    "print(len(texts))\n",
    "\n",
    "assert len(imgs) > 26 # can be more than that due to visitor icons\n",
    "print(len(imgs))\n",
    "\n",
    "assert any(\"Navigable\" in value for value in texts.values()) # word is on a slide\n",
    "assert any(\"MINUS\" in value for value in texts.values()) # word is in speaker notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-oQXhO2vTTv",
    "outputId": "856c12e5-ee82-4169-d812-ee8c409d1f50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for:  architecture\n",
      "\t ['grant-translate.txt', 'cs.pdf', 'deep-features-scene (1).pdf', 'Presentation: 4. Vector space modelling with ML - Google Презентации, Slide No: 5', 'Presentation: 4. Vector space modelling with ML - Google Презентации, Slide No: 16', 'Presentation: 4. Vector space modelling with ML - Google Презентации, Slide No: 19', 'Presentation: 7. Basics of web - Google Презентации, Slide No: 51']\n",
      "Results for:  algorithm\n",
      "\t ['DSA_15 Lion in the desert.pptx', '[DM]-Course Description.docx', 'Tutorial #8.pdf', 'grant-translate.txt', 'retake-2016-08-18.docx', 'cs.pdf', 'Tutorial 9.pdf', 'grant.txt', 'sort.js', 'deep-features-scene (1).pdf', 'DSA_09 - 2-3-4 and B-Trees.pdf', 'at least this file.txt', 'dsa.pdf', 'Presentation: 6. Approximate nearest neighbours search 2. Trees - Google Презентации, Slide No: 13', 'Presentation: 6. Approximate nearest neighbours search 2. Trees - Google Презентации, Slide No: 14', 'Presentation: 6. Approximate nearest neighbours search 2. Trees - Google Презентации, Slide No: 32', 'Presentation: 6. Approximate nearest neighbours search 2. Trees - Google Презентации, Slide No: 34', 'Presentation: 7. Basics of web - Google Презентации, Slide No: 20']\n",
      "Results for:  function\n",
      "\t ['DSA_15 Lion in the desert.pptx', 'neuro.html', '[DM]-Course Description.docx', 'bloomset.js', 'Tutorial #8.pdf', 'grant-translate.txt', 'retake-2016-08-18.docx', 'FuncnNEW.pdf', 'Assessment Criteria (May).pdf', 'sort.js', 'deep-features-scene (1).pdf', 'at least this file.txt', 'dsa.pdf', 'Presentation: 6. Approximate nearest neighbours search 2. Trees - Google Презентации, Slide No: 10', 'Presentation: 6. Approximate nearest neighbours search 2. Trees - Google Презентации, Slide No: 11', 'Presentation: 7. Basics of web - Google Презентации, Slide No: 32']\n",
      "Results for:  dataset\n",
      "\t ['Small dataset face recognition.pptx', 'grant-translate.txt', 'students.txt', 'grant.txt', 'deep-features-scene (1).pdf', 'at least this file.txt', 'Presentation: 4. Vector space modelling with ML - Google Презентации, Slide No: 9', 'Presentation: 6. Approximate nearest neighbours search 2. Trees - Google Презентации, Slide No: 23', 'Presentation: 6. Approximate nearest neighbours search 2. Trees - Google Презентации, Slide No: 27']\n",
      "Results for:  Protasov\n",
      "\t ['3cases.pdf', 'ai-junior.pdf', 'students.txt', 'MS2 - Problems of multithread programming.pptx', 'cs.pdf', 'grant.txt', 'deep-features-scene (1).pdf', 'at least this file.txt', 'dsa.pdf', 'Presentation: 4. Vector space modelling with ML - Google Презентации, Slide No: 1', 'Presentation: 6. Approximate nearest neighbours search 2. Trees - Google Презентации, Slide No: 1', 'Presentation: 7. Basics of web - Google Презентации, Slide No: 1', 'Presentation: 7. Basics of web - Google Презентации, Slide No: 10', 'Presentation: 7. Basics of web - Google Презентации, Slide No: 17']\n",
      "Results for:  cosine\n",
      "\t ['Presentation: 4. Vector space modelling with ML - Google Презентации, Slide No: 16', 'Presentation: 6. Approximate nearest neighbours search 2. Trees - Google Презентации, Slide No: 1']\n",
      "Results for:  модель\n",
      "\t ['3cases.pdf', 'grant-translate.txt', 'Presentation: 4. Vector space modelling with ML - Google Презентации, Slide No: 16', 'Presentation: 7. Basics of web - Google Презентации, Slide No: 49']\n",
      "Results for:  например\n",
      "\t ['3cases.pdf', 'grant-translate.txt', 'Presentation: 4. Vector space modelling with ML - Google Презентации, Slide No: 4', 'Presentation: 7. Basics of web - Google Презентации, Slide No: 11', 'Presentation: 7. Basics of web - Google Презентации, Slide No: 18', 'Presentation: 7. Basics of web - Google Презентации, Slide No: 36', 'Presentation: 7. Basics of web - Google Презентации, Slide No: 51']\n"
     ]
    }
   ],
   "source": [
    "queries = [\"architecture\", \"algorithm\", \"function\", \"dataset\", \n",
    "           \"Protasov\", \"cosine\", \"модель\", \"например\"]\n",
    "\n",
    "for query in queries:\n",
    "    r = find(query)\n",
    "    print(\"Results for: \", query)\n",
    "    print(\"\\t\", r)\n",
    "    assert len(r) > 0, \"Query should return at least 1 document\"\n",
    "    assert len(r) > 1, \"Query should return at least 2 documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsCzk-Y639ix"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2021S_0304_GDrive_magic.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}